{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff2a6ab-a52e-4cb3-8f6d-97c0f700d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a9a40-c166-471c-aff4-9af0d22e9bfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Levene Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37851ed-d143-45ca-abf5-91cec7e52bf5",
   "metadata": {},
   "source": [
    "Youtube: https://www.youtube.com/watch?v=x51GDTiPIfI <br/>\n",
    "\n",
    "The Levene test, named after its developer Howard Levene, is a statistical test used to assess whether the variances of two or more groups or samples are equal or significantly different from each other. It is often used as a preliminary step in statistical analysis to determine whether the assumption of equal variances (homoscedasticity) is met before conducting certain parametric statistical tests, such as the t-test or analysis of variance (ANOVA).\n",
    "\n",
    "Here's how the Levene test works:\n",
    "\n",
    "1. Formulate the null hypothesis (H0): The null hypothesis in the Levene test is that the variances of the groups or samples being compared are equal.\n",
    "\n",
    "2. Formulate the alternative hypothesis (H1): The alternative hypothesis is that the variances of the groups or samples are not equal.\n",
    "\n",
    "3. Calculate a test statistic: The Levene test calculates a test statistic by comparing the absolute deviations of individual data points from their respective group or sample means.\n",
    "\n",
    "4. Determine the p-value: The test statistic is then used to calculate a p-value, which represents the probability of obtaining the observed variance differences (or more extreme differences) if the null hypothesis were true.\n",
    "\n",
    "5. Make a decision: Based on the calculated p-value and a chosen significance level (e.g., α = 0.05), you can decide whether to reject the null hypothesis. If the p-value is less than the chosen significance level, you reject the null hypothesis, indicating that there is significant evidence that the variances are not equal. If the p-value is greater than the significance level, you fail to reject the null hypothesis, suggesting that there is not enough evidence to conclude that the variances are different.\n",
    "\n",
    "If the Levene test indicates that the variances are significantly different, you may need to use alternative statistical methods that do not assume equal variances, such as Welch's t-test or non-parametric tests like the Mann-Whitney U test or Kruskal-Wallis test.\n",
    "\n",
    "Overall, the Levene test is a useful tool for assessing the assumption of equal variances, which is important for the validity of various statistical analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78aa6c2-b500-48c6-9ad3-c39bc35f3a58",
   "metadata": {},
   "source": [
    "### Problem\n",
    "Suppose you are a researcher studying the performance of three different teaching methods (A, B, and C) on a group of students. You want to determine if there is a significant difference in the variances of test scores achieved by students in these three teaching methods. Your null hypothesis is that the variances are equal, and your alternative hypothesis is that they are not equal.\n",
    "\n",
    "You have the following data for each teaching method:\n",
    "\n",
    "Teaching Method A:\n",
    "[85, 88, 90, 92, 87, 89]\n",
    "\n",
    "Teaching Method B:\n",
    "[82, 81, 80, 84, 79, 85]\n",
    "\n",
    "Teaching Method C:\n",
    "[88, 85, 82, 81, 87, 84]\n",
    "\n",
    "Using Python, you want to perform Levene's test to test the equality of variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de30f178-4ce8-4caa-acd7-1b900efdad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levene's Test:\n",
      "Test Statistic: 0.12820512820512814\n",
      "P-Value: 0.8806265049681821\n",
      "Fail to reject the null hypothesis. There is no significant evidence that the variances are different.\n"
     ]
    }
   ],
   "source": [
    "# Solution Using Python\n",
    "# Data for each teaching method\n",
    "teaching_method_A = [85, 88, 90, 92, 87, 89]\n",
    "teaching_method_B = [82, 81, 80, 84, 79, 85]\n",
    "teaching_method_C = [88, 85, 82, 81, 87, 84]\n",
    "\n",
    "# Perform Levene's Test\n",
    "statistic, p_value = stats.levene(teaching_method_A, teaching_method_B, teaching_method_C)\n",
    "\n",
    "# Set the significance level (alpha)\n",
    "alpha=0.05\n",
    "\n",
    "# Print the results\n",
    "print(\"Levene's Test:\")\n",
    "print(\"Test Statistic:\", statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Decide whether to reject the null hypothesis\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. There is evidence that the variances are not equal.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant evidence that the variances are different.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc06ef4-bb87-4c7e-b6ad-919a099712ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77c0d171-d017-4b58-b93c-1632403c0f0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Shapiro Wilk Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff6645b-c24a-491f-b89b-cefc2e512659",
   "metadata": {},
   "source": [
    "The Shapiro-Wilk test, named after its developers Samuel Shapiro and Martin Wilk, is a statistical test used to assess whether a given dataset follows a normal distribution or not. It is one of the methods used to check the normality assumption, which is important for many statistical techniques, such as parametric tests like t-tests and analysis of variance (ANOVA).\n",
    "\n",
    "Here's how the Shapiro-Wilk test works:\n",
    "\n",
    "1. **Null Hypothesis (H0):** The null hypothesis of the Shapiro-Wilk test is that the data follows a normal distribution.\n",
    "\n",
    "2. **Alternative Hypothesis (H1):** The alternative hypothesis is that the data does not follow a normal distribution.\n",
    "\n",
    "3. **Test Statistic:** The test calculates a statistic (W) that measures the degree of departure from normality. This statistic is based on the differences between the observed sample data and the values expected if the data were normally distributed.\n",
    "\n",
    "4. **P-Value:** The test produces a p-value, which indicates the probability of obtaining the observed departure from normality (or more extreme deviations) if the null hypothesis were true. A small p-value (typically less than the chosen significance level, often 0.05) suggests evidence to reject the null hypothesis, indicating that the data is not normally distributed. Conversely, a larger p-value suggests that there is insufficient evidence to conclude that the data departs significantly from a normal distribution.\n",
    "\n",
    "5. **Decision:** Based on the calculated p-value and a chosen significance level (alpha), you can decide whether to reject the null hypothesis or not. If p-value < alpha, you reject the null hypothesis, concluding that the data is not normally distributed. If p-value ≥ alpha, you fail to reject the null hypothesis, suggesting that the data may reasonably follow a normal distribution.\n",
    "\n",
    "It's important to note that the Shapiro-Wilk test is most useful for relatively small to moderately sized datasets (typically less than 5,000 observations). For larger datasets, even minor departures from normality can lead to the test detecting non-normality as statistically significant due to its high power.\n",
    "\n",
    "In practice, the Shapiro-Wilk test is often used in conjunction with visual methods like histograms, Q-Q plots, and normal probability plots to assess the normality assumption of a dataset before applying parametric statistical tests that assume normality. If the data significantly deviates from normality, alternative non-parametric tests or transformations of the data may be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c94e6-cdc7-4d23-be53-217e6dd0efd6",
   "metadata": {},
   "source": [
    "### Problem:\n",
    "\n",
    "A manufacturer of light bulbs is concerned about the quality of its products. They suspect that the lifespan (in hours) of a certain type of light bulb they produce may not be normally distributed. To investigate this, they randomly select a sample of 30 light bulbs from their production line and record their lifespans. The company wants to determine whether the lifespans of these light bulbs follow a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dde3977-9559-418f-9c0e-544bd0de91ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk Test:\n",
      "Test Statistic: 0.9725183248519897\n",
      "P-Value: 0.610126256942749\n",
      "Fail to reject the null hypothesis. There is no significant evidence that the lifespans are not normally distributed.\n"
     ]
    }
   ],
   "source": [
    "# Solve this problem\n",
    "# Lifespan data of 30 randomly selected light bulbs\n",
    "lifespans = np.array([1100, 1150, 1125, 1080, 1200, 1225, 1175, 1180, 1230, 1195,\n",
    "                      1220, 1165, 1210, 1240, 1170, 1120, 1135, 1215, 1190, 1245,\n",
    "                      1160, 1145, 1125, 1205, 1155, 1235, 1190, 1185, 1210, 1140])\n",
    "\n",
    "# Perform the Shapiro-Wilk test\n",
    "statistic, p_value = stats.shapiro(lifespans)\n",
    "\n",
    "# Print the results\n",
    "print(\"Shapiro-Wilk Test:\")\n",
    "print(\"Test Statistic:\", statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. The lifespans do not follow a normal distribution.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant evidence that the lifespans are not normally distributed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ba3ea-0a6e-4e42-ab8a-d08451453995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c06491a-ba28-4909-ae93-4499d5ce899b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# K-S Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ddc2b8-f1ec-47b9-b2a1-a17d8dc93a3e",
   "metadata": {},
   "source": [
    "The Kolmogorov-Smirnov (K-S) test is a non-parametric statistical test used to assess whether a given dataset follows a particular probability distribution or if two datasets are drawn from the same underlying distribution. It is named after Andrey Kolmogorov and Nikolai Smirnov, who developed the test.\n",
    "\n",
    "The K-S test is used to compare an observed dataset to a reference (theoretical) distribution or to compare two datasets to determine if they come from the same underlying population distribution. It is particularly useful when you either don't know the specific distribution of your data or suspect that it may deviate from a known distribution.\n",
    "\n",
    "There are two main variants of the K-S test:\n",
    "\n",
    "1. **One-sample K-S test:** This version of the test is used to compare a single dataset to a known theoretical distribution. The null hypothesis (H0) assumes that the dataset follows the specified theoretical distribution. The test calculates a test statistic (D) that represents the maximum absolute difference between the empirical cumulative distribution function (ECDF) of the observed data and the cumulative distribution function (CDF) of the theoretical distribution. The test also produces a p-value, which indicates the probability of observing such a maximum difference under the null hypothesis.\n",
    "\n",
    "2. **Two-sample K-S test:** This version of the test is used to compare two independent datasets to determine if they come from the same underlying population distribution. The null hypothesis (H0) assumes that the two datasets are drawn from the same distribution. Like the one-sample test, it calculates a test statistic (D) representing the maximum absolute difference between the ECDFs of the two datasets and produces a p-value.\n",
    "\n",
    "Here's a summary of the steps for both versions of the K-S test:\n",
    "\n",
    "**One-sample K-S test:**\n",
    "\n",
    "1. Formulate the null hypothesis (H0): The data follows the specified theoretical distribution.\n",
    "\n",
    "2. Formulate the alternative hypothesis (H1): The data does not follow the specified theoretical distribution.\n",
    "\n",
    "3. Calculate the test statistic (D).\n",
    "\n",
    "4. Determine the p-value, representing the probability of observing D or a larger value under H0.\n",
    "\n",
    "5. Compare the p-value to a chosen significance level (alpha) to make a decision. If p-value < alpha, reject H0, indicating that the data does not follow the specified distribution.\n",
    "\n",
    "**Two-sample K-S test:**\n",
    "\n",
    "1. Formulate the null hypothesis (H0): The two datasets are drawn from the same underlying distribution.\n",
    "\n",
    "2. Formulate the alternative hypothesis (H1): The two datasets are not drawn from the same underlying distribution.\n",
    "\n",
    "3. Calculate the test statistic (D) by comparing the ECDFs of the two datasets.\n",
    "\n",
    "4. Determine the p-value, representing the probability of observing D or a larger value under H0.\n",
    "\n",
    "5. Compare the p-value to a chosen significance level (alpha) to make a decision. If p-value < alpha, reject H0, indicating that the two datasets come from different distributions.\n",
    "\n",
    "In Python, you can perform the K-S test using libraries like SciPy (for one-sample and two-sample tests) and numpy (for data manipulation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc0d37c-2d5d-40bf-a409-ab4fe0556713",
   "metadata": {},
   "source": [
    "### Problem One Sample K-S Test:\n",
    "\n",
    "A manufacturer of computer chips claims that their chips have a consistent and normally distributed processing time with a mean processing time of 24 milliseconds. To verify this claim, a quality control manager randomly selects 40 computer chips from the production line and records their processing times. The recorded processing times are as follows:\n",
    "\n",
    "[22.5, 25.2, 23.8, 26.1, 24.9, 23.0, 25.7, 24.4, 23.2, 26.0, 24.3, 25.6, 23.7, 25.1, 24.8, 26.2, 23.9, 25.0, 24.6, 26.4, 23.5, 25.3, 24.7, 26.3, 23.6, 25.5, 24.2, 26.5, 23.1, 25.4, 24.5, 25.8, 24.1, 26.6, 23.3, 25.9, 24.0, 26.7]\n",
    "\n",
    "Perform a one-sample Kolmogorov-Smirnov (K-S) test to determine whether the processing times of these computer chips follow a normal distribution at a significance level of 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8916a77a-7750-4b07-8dba-cb2eff167726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Sample Kolmogorov-Smirnov (K-S) Test:\n",
      "Test Statistic: 0.2857645374741111\n",
      "P-Value: 0.003064886737276562\n",
      "Reject the null hypothesis. The processing times do not follow a normal distribution.\n"
     ]
    }
   ],
   "source": [
    "# Solve this problem\n",
    "# Recorded processing times\n",
    "processing_times = np.array([22.5, 25.2, 23.8, 26.1, 24.9, 23.0, 25.7, 24.4, 23.2, 26.0,\n",
    "                              24.3, 25.6, 23.7, 25.1, 24.8, 26.2, 23.9, 25.0, 24.6, 26.4,\n",
    "                              23.5, 25.3, 24.7, 26.3, 23.6, 25.5, 24.2, 26.5, 23.1, 25.4,\n",
    "                              24.5, 25.8, 24.1, 26.6, 23.3, 25.9, 24.0, 26.7])\n",
    "\n",
    "# Assumed mean and standard deviation for a normal distribution\n",
    "mu = 24 # Mean processing time according to the claim\n",
    "sigma = np.std(processing_times) # Sample Standard Deviation\n",
    "\n",
    "# Perform the one-sample K-S test\n",
    "statistic, p_value = stats.kstest(processing_times, 'norm', args=(mu, sigma))\n",
    "\n",
    "# Set the significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Print the results\n",
    "print(\"One-Sample Kolmogorov-Smirnov (K-S) Test:\")\n",
    "print(\"Test Statistic:\", statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Decide whether to reject the null hypothesis\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. The processing times do not follow a normal distribution.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant evidence that the processing times deviate from a normal distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5ad58-cff2-403e-b6b5-a2e123526099",
   "metadata": {},
   "source": [
    "### Problem Two Sample K-S Test:\n",
    "\n",
    "A company manufactures two types of batteries, Type A and Type B, both of which are supposed to have consistent and normally distributed lifetimes. To assess the quality of these batteries, a quality control manager selects random samples of each type and records their lifetimes. The recorded lifetimes (in hours) are as follows:\n",
    "\n",
    "Type A Batteries:\n",
    "[420, 425, 430, 435, 440, 445, 450, 455, 460, 465]\n",
    "\n",
    "Type B Batteries:\n",
    "[410, 415, 420, 425, 430, 435, 440, 445, 450, 455]\n",
    "\n",
    "Perform a two-sample Kolmogorov-Smirnov (K-S) test to determine whether the lifetimes of Type A and Type B batteries come from the same underlying distribution at a significance level of 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2992e4-3496-4d7d-b759-c0eacba00156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Sample Kolmogorov-Smirnov (K-S) Test:\n",
      "Test Statistic: 0.2\n",
      "P-Value: 0.9944575548290717\n",
      "Fail to reject the null hypothesis. There is no significant evidence that the lifetimes of Type A and Type B batteries come from different distributions.\n"
     ]
    }
   ],
   "source": [
    "# Solve this Problem\n",
    "# Lifetimes of Type A and Type B batteries\n",
    "type_a_lifetimes = np.array([420, 425, 430, 435, 440, 445, 450, 455, 460, 465])\n",
    "type_b_lifetimes = np.array([410, 415, 420, 425, 430, 435, 440, 445, 450, 455])\n",
    "\n",
    "# Perform the two-sample K-S test\n",
    "statistic, p_value = stats.ks_2samp(type_a_lifetimes, type_b_lifetimes)\n",
    "\n",
    "# Set the significance level (alpha)\n",
    "alpha = 0.05\n",
    "\n",
    "# Print the results\n",
    "print(\"Two-Sample Kolmogorov-Smirnov (K-S) Test:\")\n",
    "print(\"Test Statistic:\", statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Decide whether to reject the null hypothesis\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis. The lifetimes of Type A and Type B batteries come from different distributions.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. There is no significant evidence that the lifetimes of Type A and Type B batteries come from different distributions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59977112-300e-4a34-9358-bfc71d71a336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0e5dee8-c416-424c-8268-a986c46871ee",
   "metadata": {},
   "source": [
    "# Fisher's Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337f745-05b2-4664-a784-090c14923568",
   "metadata": {},
   "source": [
    "Youtube: https://www.youtube.com/watch?v=udyAvvaMjfM\n",
    "\n",
    "Fisher's test, also known as Fisher's exact test, is a statistical significance test used to determine if there are nonrandom associations between two categorical variables. It is named after its developer, Sir Ronald A. Fisher, who introduced the test in the early 20th century. Fisher's test is particularly valuable when dealing with small sample sizes or when the assumptions of other statistical tests like the chi-squared test are not met.\n",
    "\n",
    "This test is often used in 2x2 contingency tables, which are used to examine the relationships between two categorical variables. These variables are typically organized into rows and columns of the table. The goal of Fisher's test is to determine whether there is a significant association between the two variables or if the proportions of observations in different categories of one variable are independent of the categories of the other variable.\n",
    "\n",
    "Here are the fundamental steps involved in performing Fisher's exact test:\n",
    "\n",
    "1. Set up a 2x2 contingency table displaying the observed frequencies of the categories for the two variables. The table typically looks like this:\n",
    "\n",
    "```\n",
    "                  Variable 1\n",
    "               | Category A | Category B |\n",
    "-----------------------------------------\n",
    "Variable 2 |    a         |    b         |\n",
    "-----------------------------------------\n",
    "```\n",
    "\n",
    "2. Calculate the probability of observing the data under the assumption of independence, which means assuming that there is no association between the two variables. This step involves computing the probabilities of all possible tables that could be generated under the independence assumption, using a hypergeometric probability distribution.\n",
    "\n",
    "3. Determine if the observed table is significantly different from the expected table under the assumption of independence. This is accomplished by comparing the observed probability to the probabilities of all possible tables. If the observed probability is very low (indicating that the observed table is unlikely under the assumption of independence), it suggests a significant association between the two variables.\n",
    "\n",
    "4. Calculate a p-value, which represents the probability of obtaining a table as extreme as, or more extreme than, the observed table, assuming the null hypothesis (independence) is true. If the p-value is below a predetermined significance level (e.g., 0.05), you reject the null hypothesis and conclude that there is a significant association between the two variables.\n",
    "\n",
    "Fisher's exact test is frequently employed in fields such as biology, epidemiology, and the social sciences, where small sample sizes or infrequent events make other statistical tests less suitable. It provides a means to assess the significance of associations in contingency tables while accounting for data limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a8593-b04c-44c7-b0e1-9429a05262ce",
   "metadata": {},
   "source": [
    "### Problem:\n",
    "\n",
    "A medical researcher wants to investigate whether there is an association between two treatments (Treatment A and Treatment B) and the recovery status of patients (Recovered or Not Recovered) after a certain period. The researcher collected data on 50 patients who received Treatment A and 50 patients who received Treatment B. The data is summarized in the following contingency table:\n",
    "\n",
    "```\n",
    "                Treatment A    Treatment B\n",
    "Recovered       20             10\n",
    "Not Recovered   30             40\n",
    "```\n",
    "\n",
    "Perform Fisher's exact test to determine if there is a significant association between the type of treatment and the recovery status of patients at a significance level of 0.05.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9c933d-2fee-4a2f-bbc2-f0d7417758b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odds Ratio: 2.67\n",
      "P-value: 0.0486\n",
      "The association between treatment and recovery is significant at alpha = 0.05.\n"
     ]
    }
   ],
   "source": [
    "# Solve this problem\n",
    "# Create a 2x2 contingency table\n",
    "observed_data = [[20, 10], [30, 40]]\n",
    "\n",
    "# Perform Fisher's exact test\n",
    "odds_ratio, p_value = stats.fisher_exact(observed_data)\n",
    "\n",
    "# Set the significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Check if the p-value is less than alpha to determine significance\n",
    "if p_value < alpha:\n",
    "    result = \"significant\"\n",
    "else:\n",
    "    result = \"not significant\"\n",
    "\n",
    "# Display the results\n",
    "print(f\"Odds Ratio: {odds_ratio:.2f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"The association between treatment and recovery is {result} at alpha = {alpha}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c8026-84a7-426a-bc99-d89c4706ca89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
